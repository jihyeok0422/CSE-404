{
  "cells": [
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "## Part 1: Look at the Big Picture\n**Summary:** In this mini-project we will use California census data to build a model of housing prices. Our dataset is the CA Housing Prices Dataset from the StatLib repository, which is based on the 1990 census. Some of the features in this dataset are population, income, and housing prices for each district. \n\n**Goal**: Your ML model should learn from this dataset in order to predict the median housing price in any district. \n\n**Where to begin?** Some common questions at work might be: How will this model be used? How will it benefit the company or people using it? What does the current (if any) solution look like (e.g., are people hand calculating this result and if so, how)?\n\n**Frame the problem:** Is it supervised, unsupervised, hybrid, or reinforcement learning? Is it a classification or regression task, or something else? Should you use batch or online learning? What assumptions have you or others made about this task? (Note: we haven't covered all these concepts yet; this is just to give you an idea of how you can start thinking about setting up ML projects).\n\n**Task:** We are given labeled training examples, so this will be supervised learning.\nBecause we are asked to predict a value (median housing price), this is a regression task. Specifically, this is a multiple regression task because we will use many features to make our prediction. We haven't covered this in class yet, but the next step is to choose our *performance measure* -- the typical performance measure for regression tasks is the *Root Mean Square Error (RMSE)* cost function. Basically, this tells us how much error our model makes in its predictions. "
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "## Part 2: Load the Data"
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# import packages\nimport os\nimport tarfile\nimport urllib\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error",
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "## Part 2: Get the Data\n# Create a function to download housing.tgz: a comma-separated values (CSV) file representing the dataset\n\n# Specify URL & path names\nDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n\n# Creates 'datasets/housing' dir in workspace\n# Downloads & extracts housing.tgz\ndef get_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    os.makedirs(housing_path, exist_ok=True)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "get_housing_data()",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Load the data using pandas\n# Returns a pandas DataFrame object containing all the data\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)\n",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "## Part 3: Visualize the Data"
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Peak at the dataset using DataFrame's head() method to return the top 5 rows\nhousing = load_housing_data()\nhousing.head()\n",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "**About the Housing Dataset** \n\nEach row represents one district. \nThere are 10 features: longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value, ocean_proximity. \n\n**TODO**: Call the *info()* method on the Housing dataset. Do you notice anything about total_bedrooms and ocean_proximity? "
    },
    {
      "metadata": {
        "scrolled": true,
        "state": "graded",
        "deletable": false,
        "id": "blue_mani",
        "starter_code": "",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "**TODO**: Check the categories and number of districts in each for the *ocean_proximity* feature by using the *value_counts( )* method. Note: You can select a feature by name and use it just as you would an index into an array. For example: housing[\"latitude\"]. "
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "housing[\"latitude\"]",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "state": "graded",
        "deletable": false,
        "id": "apt_hel",
        "starter_code": "",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "**TODO:** Now try calling the *describe( )* method on the housing dataset. What does this return? "
    },
    {
      "metadata": {
        "state": "graded",
        "deletable": false,
        "id": "spicy_oor",
        "starter_code": "",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "### Part 3a: Histograms\nAnother visualization option is to plot a histogram. A histogram will show the number of instances (y-axis) that have a given value range (x-axis). \n\nSome things to notice about the histograms:\n<ol>\n    <li> Median income: has been **preprocessed** -- scaled and capped with new range: [0.5..15]. This means that a value of 5 represents about $50,000.  </li>\n    <li> Median age has also been capped. </li>\n    <li> PROBLEM! Median house value was also capped! But this is our target attribute (our labels). When will this be a problem? </li>\n    <li> The attributes have different scales. This could be a problem later. </li>\n    <li> Several of these histograms are **tail-heavy** -- they extend farther to the right of the median than the left. This could make it harder for some ML algorithms to learn patterns. How do you think we can get around this? </li> \n</ol>"
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline \nhousing.hist(bins=50, figsize=(20,15))\nplt.show()",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "### Part 3b: Create a Test Set \nBefore creating more visualizations, let's set aside a portion of our dataset for testing. Typically, we can visualize the data to search for patterns and guess which models and features will be a good starting point. However, we don't want to introduce *data snooping bias*. In order to avoid this, we can reserve the testing set now, before loading more visualizations. That way we'll only visualize the training set that our ML algorithm will learn from. The most common data split is 80% training and 20% testing (test_size = 0.2). Another common split is 70% training, 20% testing, and 10% development/validation. "
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Split the dataset into a training set and testing set \ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "Most people use the above code to randomly sample and split their datasets. Ideally however, we should split using stratified sampling. In this approach, the population would be divided into homogenous subgroups (strata) and the correct number of instances will be sampled from each stratum (or group) to ensure that the test set represents the entire population (in the entire dataset). \n\nFor the housing prediction task, experts say that median income is the most important feature for predicting the median housing price (our prediction goal). So we can separate the median income into categories (strata) and then select the appropriate number of samples from each category to ensure correct representation in our training/testing sets. "
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Create income_cat feature with 5 categories \nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"], bins = [0., 1.5, 3.0, 4.5, 6., np.inf], labels = [1, 2, 3, 4, 5])\nhousing[\"income_cat\"].hist()",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "Now we can do stratified sampling on the income category. "
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Perform stratified sampling on income_cat to create 80% training & 20% testing sets\nsplit = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]): \n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]\n\n# Check income category proportions\nstrat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n\n    ",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Remove income_cat attribute to return dataset back to original \nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis = 1, inplace = True)\n",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "### Part 3c: Back to Visualizations\nWe'll save a copy of our training set and use it to create more data visualizations. "
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "housing = strat_train_set.copy()",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Use latitude & longitude to create a geographical scatterplot of all districts\nhousing.plot(kind = \"scatter\", x = \"longitude\", y = \"latitude\", alpha = 0.1)",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "state": "graded",
        "deletable": false,
        "id": "big_vali",
        "starter_code": "### TODO ###\n# What happens when you remove the alpha parameter in the code above? \n",
        "trusted": false
      },
      "cell_type": "code",
      "source": "### TODO ###\n# What happens when you remove the alpha parameter in the code above? \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Are housing prices related to location & population density? \n\n# param s: radius of each circle represents district's population\n# param c: color represents price\n# param cmap jet: use a predefined color map called jet (uses blue (low prices) to red (high prices))\nhousing.plot(kind = \"scatter\", x = \"longitude\", y = \"latitude\", alpha = 0.4,\n            s = housing[\"population\"]/100, label = \"population\", figsize = (10,7),\n            c = \"median_house_value\", cmap = plt.get_cmap(\"jet\"), colorbar = True)\nplt.legend()",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Use pandas to check which attributes/features are correlated with median housing value\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize = (12,8))",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Take a closer look at median house value and median income\n# What do you think the plotted horizontal lines represent? Will they be a problem?\nhousing.plot(kind = \"scatter\", x = \"median_income\", y = \"median_house_value\", alpha = 0.1)\n",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "## Part 4: Prepare Your Data for ML Algorithms\n\nFirst, we'll revert to our clean training set. Then we'll separate predictors (features, attributes, x) and labels (output, target, y). "
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "housing = strat_train_set.drop(\"median_house_value\", axis = 1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()\n",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "### Part 4a: Handle Missing Values\nAt the beginning of Part 3, you might have noticed that *total_bedrooms* was missing some values. Some ML algorithms don't work well when features are missing, so we need to fix this before training. There are 3 common approaches for how to handle missing values:\n<ol>\n    <li> Get rid of the corresponding sample. </li>\n    <li> Get rid of the whole feature. </li>\n    <li> Set the missing values to some value: zero, the mean, median, etc. </li>\n</ol>\n\nThis processing can be done using DataFrame's methods or with Scikit-Learn. "
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Create a SimpleImputer\n# Replace each attribute's missing values with the median of that attribute\nimputer = SimpleImputer(strategy = \"median\")\n\n# Because we can only compute median on numerical attributes\n# Copy the data without the ocean_proximity categories (because they're text)\nhousing_num = housing.drop(\"ocean_proximity\", axis = 1)\n\n# Fit the imputer to the training data\n# Imputer computes median of each attribute & stores result in its statistics_ instance variable\nimputer.fit(housing_num)\n\nprint(imputer.statistics_)\n#print(housing_num.median().values)",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Use the trained Imputer to transform training set \n# by replacing missing values with learned medians\n# Returns NumPy array containing transformed features\nX = imputer.transform(housing_num)\nprint(X)",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Put X back into a DataFrame\nhousing_tr = pd.DataFrame(X, columns = housing_num.columns, index = housing_num.index)\nprint(housing_tr)",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "### Part 4b: Text to Numbers\nMost ML algorithms prefer to work with numbers instead of text, so we need to convert the *ocean_proximity* text into numerical categories. "
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Look at the ocean_proximity attribute/feature\nhousing_cat = housing[[\"ocean_proximity\"]]\nprint(housing_cat.head(10))",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Represent text categories as numerical categories\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n\nprint(housing_cat_encoded[:10])\n",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Print list of categories \nprint(ordinal_encoder.categories_)",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "One problem with this type of representation is that some ML algorithms will assume nearby values are more similar than distant values, which isn't the case for *ocean_proximity* (e.g., in this case categories 0 & 4 are more similar than categories 0 & 1). \n\nTo fix this, we can use a common ML representation called **one-hot encoding**. For this type of feature representation, we create one binary attribute per category, for example: one attribute equal to 1 when the category is INLAND and 0 otherwise, another attribute equal to 1 when the category is NEAR OCEAN and 0 otherwise, and so on for each. "
    },
    {
      "metadata": {
        "state": "graded",
        "deletable": false,
        "id": "red_sif",
        "starter_code": "### TODO ###\n# Use Scikit-Learn's OneHotEncoder to convert categorical values into one-hot vectors\n\n# Create a OneHotEncoder\ncat_encoder = \n\n# Call the OneHotEncoder's fit_transform method on housing_cat\nhousing_cat_1hot = cat_encoder.\n\n# Returns a SciPy sparse matrix\nprint(housing_cat_1hot)",
        "trusted": false
      },
      "cell_type": "code",
      "source": "### TODO ###\n# Use Scikit-Learn's OneHotEncoder to convert categorical values into one-hot vectors\n\n# Create a OneHotEncoder\ncat_encoder = \n\n# Call the OneHotEncoder's fit_transform method on housing_cat\nhousing_cat_1hot = cat_encoder.\n\n# Returns a SciPy sparse matrix\nprint(housing_cat_1hot)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "state": "graded",
        "deletable": false,
        "id": "stoic_saga",
        "starter_code": "### TODO ###\n# Print the list of categories using the encoder's categories_ instance variable\nprint( )",
        "trusted": false
      },
      "cell_type": "code",
      "source": "### TODO ###\n# Print the list of categories using the encoder's categories_ instance variable\nprint( )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "### Part 4c: Feature Scaling & Transformation Pipelines\nFeature Scaling is the most common data cleaning or preprocessing step you will perform. ML algorithms don't perform well when the input numerical values for attributes have different scales. There are 2 common approaches to get all features to have the same scale: \n<ol>\n    <li> Normalization: also called min-max scaling. Values are shifted and rescaled to range from 0..1. Scikit-Learn uses the *MinMaxScaler* class for this. </li>\n    <li> Standardization. This approach doesn't force values to fall within a specific range, which can be a problem for some algorithms. However, it's less affected by outliers. Scikit-Learn uses the *StandardScaler* class for this.  </li>\n</ol>\n\nThere are several data transformation steps you might have to execute in the correct order. Scikit-Learn provides a *Pipeline* class to help with this. "
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Small pipeline for numerical attributes\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy = \"median\")),\n    ('std_scaler', StandardScaler())\n])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\nprint(housing_num_tr)",
      "execution_count": 35,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "Instead of modifying the categorical and numerical columns separately, we can use *ColumnTransformer* to apply the appropriate transformation to each column. "
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Get list of numerical column names\nnum_attribs = list(housing_num)\n\n# Get list of categorical column names \ncat_attribs = [\"ocean_proximity\"]\n\n# ColumnTransformer: numerical columns should be transformed using num_pipeline\n#                    categorical columns should be transformed using OneHotEncoder\nfull_pipeline = ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n    (\"cat\", OneHotEncoder(), cat_attribs)\n])\n\n# Apply full pipeline to housing dataset\nhousing_prepared = full_pipeline.fit_transform(housing)\nprint(housing_prepared)",
      "execution_count": 37,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "## Part 5: Select & Train a Model\nSo far we've:\n<ol>\n    <li> Framed the problem/task </li>\n    <li> Downloaded, explored, and visualized our data </li>\n    <li> Preprocessed our training and testing sets </li>\n</ol>\n\nNow we just need to choose and train our model!\nLet's use Linear Regression, which we'll be learning about soon in class. "
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Linear Regression Model\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n",
      "execution_count": 39,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Try the model on a few instances\ntest_data = housing.iloc[:5]\ntest_data_labels = housing_labels.iloc[:5]\ntest_data_prepared = full_pipeline.transform(test_data)\n\n# Predicted output (labels) by lin_reg model\nprint(\"Predictions:\", lin_reg.predict(test_data_prepared))\n\n# Actual labels\nprint(\"Labels:\", list(test_data_labels))",
      "execution_count": 41,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "And it's that easy! We fit the model and then used it to make predictions. But how good are the predictions compared to the actual labels? "
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Measure RMSE of the training set\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nprint(lin_rmse)",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "What does this RMSE mean? Is our model good or bad? Basically, this says that our typical prediction error for predicting housing price is about $69050. This is probably not ideal for a real-world application. But this is a good example of *underfitting*: likely our features didn't give us enough information to make good predictions or the model isn't powerful enough. How do you think we can handle underfitting?\n\nNote: normally we will use *cross-validation* to evaluate our models, but that will appear in future code projects.\n"
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "## Part 6: Fine-tune Your Model\nBy this step in your project, you would have tried a few models and selected the one with the best RMSE. Once you've got the best model, you then want to *fine-tune* it. Depending on the model, that could mean doing any of the following: \n\n<ol>\n    <li> Tune hyperparameters with a search algorithm, e.g., Grid Search or Randomized Search. </li>\n    <li> Ensemble approach: combine the models that perform the best. </li>\n    <li> Check and correct errors: Inspect the best performing models and check their errors. Depending on the errors you might drop less useful features, add more features, remove outliers, etc. </li>\n</ol>\n\nOnce you've got the best possible model (including learning algorithm, hyperparameters, and features), then you want to evaluate it on your test set."
    },
    {
      "metadata": {
        "state": "normal",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Get predictors & labels from test set\n# Run full_pipeline to transform data\n# Evaluate final model on test set\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = lin_reg.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\n\nprint(final_rmse)\n",
      "execution_count": 45,
      "outputs": []
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "Note: if we had chosen a different model, we could have lowered this further. "
    },
    {
      "metadata": {
        "state": "normal"
      },
      "cell_type": "markdown",
      "source": "## All done!\nAt a job you might be asked to present your results and models, including what steps you took, what worked and didn't work, assumptions, model limitations, etc. Then you'd launch your system, monitor it, and maintain it. "
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "mimir": {
      "project_id": "5f25ed4b-2c5a-4ef7-ad44-c76e81dbc191",
      "last_submission_id": "",
      "data": {}
    },
    "varInspector": {
      "window_display": false,
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "library": "var_list.py",
          "delete_cmd_prefix": "del ",
          "delete_cmd_postfix": "",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "library": "var_list.r",
          "delete_cmd_prefix": "rm(",
          "delete_cmd_postfix": ") ",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}